<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Machine Learning, Technology, and Philosophy</title>
    <link>https://talman.fi</link>
    <description>Reflections on machine learning, AI research, technology, and philosophy by Aarne Talman.</description>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Sep 2025 13:01:57 GMT</lastBuildDate>
    <pubDate>Sat, 20 Sep 2025 00:00:00 GMT</pubDate>
    <ttl>60</ttl>
    <managingEditor>aarne@talman.fi (Aarne Talman)</managingEditor>
    <webMaster>aarne@talman.fi (Aarne Talman)</webMaster>
    <atom:link href="https://talman.fi/rss.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Building an AI Research Assistant Platform</title>
      <link>https://talman.fi/posts/muves-papers.html</link>
      <guid isPermaLink="true">https://talman.fi/posts/muves-papers.html</guid>
      <pubDate>Sat, 20 Sep 2025 00:00:00 GMT</pubDate>
      <author>aarne@talman.fi (Aarne Talman)</author>
      <category>Projects</category>
      <description>How we built a platform to help researchers navigate the overwhelming volume of AI research papers using AI agents and smart organisation.</description>
      <content:encoded><![CDATA[<h1>Building an AI Research Assistant Platform</h1>
<p>The volume of AI research is growing exponentially. ArXiv alone publishes hundreds of new papers every day, making it nearly impossible for researchers, students, and practitioners to stay current with meaningful developments in their areas of interest.</p>
<p>This information overload problem led us (together with Dmitry Kan and Stergios Chatzikyriakidis) to build <strong>Muves Papers</strong> - a platform that helps people discover, organize, and understand AI research papers more effectively.</p>
<h2>The Problem We&#39;re Solving</h2>
<p>Staying current with AI research has become a significant challenge, even for experts in the field. The traditional approach of manually scanning through paper titles, abstracts, and skimming content simply doesn&#39;t scale when you&#39;re dealing with hundreds of new publications per week.</p>
<p>Researchers face several specific pain points:</p>
<p><strong>Information Overload</strong>: Too many papers to review manually, making it easy to miss important developments.</p>
<p><strong>Organisation Challenges</strong>: Difficulty organising and categorising papers by project or research area.</p>
<p><strong>Comprehension Barriers</strong>: Complex papers with dense technical content that require significant time investment to understand.</p>
<p><strong>Context Loss</strong>: Hard to see connections between papers or track how ideas evolve across multiple publications.</p>
<h2>Our Approach</h2>
<p>Muves addresses these challenges through a combination of AI-powered analysis and smart organisation tools. Muves Papers is an AI-powered research platform designed to help researchers, students, and practitioners navigate the overwhelming volume of AI literature. Instead of manually sifting through hundreds of papers, users can discover trending research, organise papers into focused projects, and leverage AI agents to analyse content and generate insights. The platform transforms the traditional research workflow from time-consuming manual processes into an efficient, AI-assisted experience.</p>
<p><img src="https://talman.fi/images/muves-papers/trending.png" alt="Muves Papers"></p>
<h2>The Platform</h2>
<p>The platform is built around five core features:</p>
<h3>Paper Analysis</h3>
<p><img src="https://talman.fi/images/muves-papers/paper-analysis.png" alt="Muves Paper Analysis"></p>
<p>Individual papers get processed through our analysis agent, which breaks down complex research into structured insights. The system extracts key findings, methodology, and significance, making it much faster to understand what a paper contributes and whether it&#39;s relevant to your work.</p>
<h3>Topic Search and Discovery</h3>
<p><img src="https://talman.fi/images/muves-papers/topic-summary.png" alt="Topic Summary Example"></p>
<p>Users can explore research areas through our topic search feature, which provides AI-generated overviews of complex domains. This helps researchers quickly get up to speed on unfamiliar areas or understand how different concepts relate to each other.</p>
<h3>Project-Based Organisation</h3>
<p><img src="https://talman.fi/images/muves-papers/workspace.png" alt="Muves Workspace"></p>
<p>The platform allows users to organise papers into focused projects, keeping related research together. This project-based approach makes it much easier to build comprehensive understanding around specific research questions or areas of interest.</p>
<h3>Automated Literature Reviews</h3>
<p><img src="https://talman.fi/images/muves-papers/literature-review.png" alt="Literature Review"></p>
<p>Once you&#39;ve collected papers in a project, our Research Assistant can automatically generate structured literature reviews. This saves significant time compared to manually synthesising information across multiple papers and helps identify patterns and gaps in the research.</p>
<h3>Chat with Project Papers</h3>
<p>The Research Assistant provides conversational access to your paper collections. You can ask questions about specific papers, request summaries of key concepts, or explore connections between different pieces of research - all scoped to the papers you&#39;ve actually added to your project.</p>
<h2>Technical Architecture</h2>
<p>Muves is built on a modern web stack with several specialised AI agents:</p>
<p><strong>Paper Analysis Agent</strong>: Processes individual papers to extract key insights, methodology, and significance.</p>
<p><strong>Topic Summarisation Agent</strong>: Creates structured overviews of research areas and emerging trends.</p>
<p><strong>Research Assistant Agent</strong>: Provides conversational access to paper collections and generates literature reviews.</p>
<p><strong>Discovery Engine</strong>: Surfaces trending and relevant papers based on community signals and research impact.</p>
<p>The platform integrates with multiple data sources including ArXiv, academic databases, and research community signals to provide comprehensive coverage of the AI research landscape.</p>
<h2>Current Status and Learning</h2>
<p>We soft-launched Muves earlier this year and have been iterating based on user feedback from researchers and practitioners. The platform currently helps users track thousands of papers across different AI domains. We&#39;re currently supported by Google Cloud credits, which allows us to keep the platform operational while we explore sustainable funding models for long-term growth.</p>
<p>Some key insights we&#39;ve learned:</p>
<p><strong>Project-based organisation is crucial</strong> - Users need to group papers by their specific research interests rather than broad categories.</p>
<p><strong>AI assistance works best when scoped</strong> - Rather than trying to answer questions about all research, focusing the AI on specific paper collections provides more accurate and useful responses.</p>
<p><strong>Context matters more than completeness</strong> - Users prefer curated, relevant content over comprehensive but noisy feeds.</p>
<p><strong>Literature reviews save significant time</strong> - Automated synthesis of project papers into structured reviews has been one of the most valuable features for users.</p>
<h2>What&#39;s Next</h2>
<p>We&#39;re continuing to develop Muves based on how researchers actually work with literature. Upcoming features include better collaboration tools for research teams, more sophisticated trend analysis, and enhanced integration with reference managers and note-taking workflows.</p>
<p>The goal remains the same: making it easier for people to stay current with AI research without being overwhelmed by the volume of new information.</p>
<p>If you&#39;re interested in trying Muves, you can check it out at <a href="https://papers.muves.io">papers.muves.io</a>. We&#39;re always looking for feedback from researchers and practitioners who deal with the challenge of staying current in fast-moving fields.</p>
<hr>
<p><em>Muves is developed by myself along with Dmitry Kan and Stergios Chatzikyriakidis, combining our experience in AI research, search technologies, and natural language processing.</em></p>
]]></content:encoded>
    </item>
    <item>
      <title>Do Language Models Actually Understand Language?</title>
      <link>https://talman.fi/posts/ai-language-understanding.html</link>
      <guid isPermaLink="true">https://talman.fi/posts/ai-language-understanding.html</guid>
      <pubDate>Sat, 13 Sep 2025 00:00:00 GMT</pubDate>
      <author>aarne@talman.fi (Aarne Talman)</author>
      <category>Machine Learning</category>
      <category>Philosophy</category>
      <description>Exploring whether language models truly understand language or are merely exceptionally good at mimicking linguistic skills through statistical patterns.</description>
      <content:encoded><![CDATA[<h1>Do language models Actually Understand Language?</h1>
<p><em>Originally posted in <a href="https://medium.com/@aarnetalman/do-ai-models-actually-understand-language-ce2f4e9a7fb9">Medium</a></em></p>
<p>By now, most people are well aware of how proficient language models like GPT-4, Claude, and Gemini are at tasks requiring various linguistic skills and knowledge. These language models can write essays, summarise complex documents, and even pass standardised tests, demonstrating impressive capabilities. But to what extent do models with such skills truly understand language, or are they merely exceptionally good at mimicking it?</p>
<p>I explored this question extensively in my <a href="http://urn.fi/URN:ISBN:978-951-51-9581-4">PhD thesis</a>, but in this blog post I aim to provide a more concise summary of the key takeaways, and hopefully nudge readers to think about these more fundamental and philosophical aspects of AI instead of the constant race to the top of the leaderboards.</p>
<h2>What Do We Mean by &quot;Understanding&quot;?</h2>
<p>Philosophers, linguists, and cognitive scientists have debated the nature of language understanding for centuries. Language understanding has traditionally been thought of a capability that only humans possess, however, recently language understanding has also been studied as a capability that machine learning models and AI systems could be said to achieve. I will not cover all the different accounts of human language understanding here, but will focus on two main perspectives that emerge from recent AI and natural language understanding (NLU) literature.</p>
<p>The two accounts are:</p>
<p><strong>The Usage-Based Definition</strong>: Understanding is the ability to use language effectively in complex tasks. This approach assumes that language understanding can be measured through various proxy tasks, like question answering (QA) or natural language inference (NLI).</p>
<p><strong>The Intent-Based Definition</strong>: Understanding goes beyond task performance. It involves grasping the speaker&#39;s intent, like their underlying goals or emotions.</p>
<p>The usage-based definition is what is often implicitly assumed in NLU research. When companies and researchers compare their models&#39; language understanding capabilities with other models, this is mostly done by comparing scores on evaluation benchmarks and tasks. Consider this example from the <a href="https://arxiv.org/abs/1810.04805">BERT paper</a>:</p>
<blockquote>
<p>In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus [...] Despite its simplicity, we demonstrate [...] that pre-training towards this task is very beneficial to both QA and NLI.</p>
</blockquote>
<p>In this example the authors implicitly assume that QA and NLI tasks are examples of tasks that require understanding, but they don&#39;t give any explicit definition of language understanding.</p>
<p>The usage-based account of language understanding is problematic in the sense that it does not really specify what capabilities a model should have in order to understand language. Is a model that performs well in text summarisation but fails in other tasks able to understand language or do different tasks test different degrees of language understanding?</p>
<p>The intent-based definition assumes that language understanding requires intents. These intents are often taken to be about a mental representation, a model of the world, and this model is used in comprehending language and in communication. Successful communication, in this view, requires a grasp of the communicative intent of the speaker. There are various formulations of the intent-based approach. One clear example can be found form the work of psychologists <a href="https://doi.org/10.3758/s13421-016-0619-6">Pettison and Radvansky</a>:</p>
<blockquote>
<p>During text comprehension, readers create mental representations of the described events, called situation models. When new information is encountered, these models must be updated or new ones created.</p>
</blockquote>
<p>The main problem with the intent-based definition is that it relies on the definition of intent, which itself is a difficult term to define. In philosophy, intention is often defined as something like &quot;the power of minds and mental states to be about, to represent, or to stand for things, properties, and states of affair&quot; <a href="https://plato.stanford.edu/entries/intention/">Setiya 2022</a>. Could we ascribe intention to language models given this definition?</p>
<h2>The Limits of Current Benchmarks</h2>
<p>Before exploring whether language models could be said to have intents, let&#39;s discuss the limitations of NLU evaluation benchmarks. Most NLU models are evaluated on benchmarks such as <a href="https://arxiv.org/abs/2009.03300v3">MMLU</a> or <a href="https://super.gluebenchmark.com/">SuperGLUE</a>. These benchmarks measure performance on specific tasks, such as natural language inference or question-answering. High scores on these tasks are often seen as evidence of language understanding.</p>
<p>However, research suggests that these benchmarks may not be reliable indicators of true understanding. Studies have shown that NLU models can perform well on these tasks even when the word order of sentences is scrambled <a href="https://arxiv.org/abs/2012.15180">Pham et al. 2020</a> or if words are removed from sentences <a href="https://arxiv.org/abs/2104.04751">Talman et al. 2021</a>, <a href="https://arxiv.org/abs/2201.04467">2022</a>. These results suggest that models might be relying on statistical patterns and shortcuts rather than deep language understanding.</p>
<h2>The Role of Intent</h2>
<p>Given that the usage-based account does not really give us a definition of what language understanding is and, moreover, given that the current tasks and benchmarks don&#39;t necessarily measure understanding, let&#39;s focus a bit more on the intent-based account.</p>
<p>The intent-based definition of understanding presents a different challenge. Can language models, which are fundamentally statistical machines, grasp the nuanced intentions behind human communication?</p>
<p>Some argue that they can. According to them, neural networks learn representations of the world from the data they&#39;re trained on. These representations could be seen as a form of understanding. When a model generates a response, it&#39;s drawing upon its learned representation of the world to predict the most appropriate answer. A clear argument in this direction is made by Ilya Sutskever in his <a href="https://blogs.nvidia.com/blog/sutskever-openai-gtc/">fireside chat</a> with the founder and CEO of NVIDIA, Jensen Huang:</p>
<blockquote>
<p>[W]hat the neural net learns is some representation of the process that produced the text, and that&#39;s a projection of the world.</p>
</blockquote>
<p>Others argue that this is not true understanding. Human understanding involves a lifetime of embodied experiences and complex cognitive processes. While language models can learn impressive representations, they might still lack the depth and richness of human understanding.</p>
<h2>A New Kind of Understanding?</h2>
<p>The rise of AI may force us to reconsider what it means to understand language.</p>
<p>Mitchell and Krakauer hypothesise in their <a href="https://arxiv.org/abs/2210.13966">recent survey</a> that AI has created new forms of understanding. Where we have earlier considered language understanding to be only of the kind that we humans possess, there likely are other forms of language understanding that could more easily be attributed to artificial systems, like NLU models.</p>
<p>It is possible that language models possess a unique form of understanding that is different from our own. This new kind of understanding might not involve emotions or consciousness, but it could still be incredibly powerful and valuable.</p>
<h2>What&#39;s Next?</h2>
<p>So where do I stand in this debate? Do I think current language models understand language? Before I can answer this question I think we need two things:</p>
<p><strong>A clear definition of what language understanding (for AI) is</strong></p>
<p><strong>Better evaluation benchmarks that capture and measure understanding according to the above definition</strong></p>
<p>The current evaluation benchmarks for natural language understanding models are clearly limited (as discussed above). To truly gauge a model&#39;s understanding, we need better evaluation methods that go beyond simple task performance. These methods should assess a model&#39;s ability to reason, use common sense knowledge, and interpret the subtle nuances of human communication.</p>
<p>We have started to explore different approaches to evaluate language models in our <a href="https://eloquent-lab.github.io">ELOQUENT Lab</a> shared task. The first results will be presented later this year at <a href="https://clef2024.clef-initiative.eu">CLEF 2024</a> and we plan to make the ELOQUENT Lab an annual shared task.</p>
<p>In addition to better evaluation benchmarks, we also need more fundamental research on the nature of language understanding and language models&#39; capabilities to understand language.</p>
]]></content:encoded>
    </item>
    <item>
      <title>Welcome to My Blog</title>
      <link>https://talman.fi/posts/welcome.html</link>
      <guid isPermaLink="true">https://talman.fi/posts/welcome.html</guid>
      <pubDate>Mon, 01 Sep 2025 00:00:00 GMT</pubDate>
      <author>aarne@talman.fi (Aarne Talman)</author>
      <category>Blogging</category>
      <description>Introducing my personal blog and the custom blogging system I built for it.</description>
      <content:encoded><![CDATA[<h1>Welcome to My Blog</h1>
<p>Hello! I&#39;m Aarne, and this is my personal blog where I&#39;ll be sharing my thoughts and experiences in the world of technology.</p>
<h2>Why This Blog Exists</h2>
<p>I&#39;ve been meaning to start writing more about my work and interests for a while now. And I&#39;ve actually started blogging a few times. However, I&#39;ve always found the blogging sites and platforms to be clunky and against my workflow. However, there&#39;s something valuable about putting thoughts into words. It helps clarify ideas, creates a record of learning, and hopefully provides value to others facing similar challenges or questions.</p>
<p>Rather than using an existing blogging platform, I decided to build my own system. It seemed like a good opportunity to create exactly what I wanted while learning something new in the process.</p>
<h2>The Blogging System</h2>
<p>This blog is powered by a custom static site generator I built specifically for my needs. Here&#39;s how it works:</p>
<p><strong>Content Organization</strong>: Each blog post lives in its own folder containing the markdown file and any associated images or assets. This keeps everything organized and makes it easy to manage posts with multiple files.</p>
<p><strong>Build Process</strong>: The system converts markdown to HTML, applies a consistent design, and generates a complete static website. Everything is processed locally on my machine.</p>
<p><strong>Deployment</strong>: The generated site deploys automatically to GitHub Pages with a single command. No server management, no databases, just fast static files served from GitHub&#39;s CDN.</p>
<p><strong>Design Philosophy</strong>: I wanted something minimal, fast-loading, and focused on readability. The dark theme is easy on the eyes and the responsive design works well across devices.</p>
<p>The entire workflow from writing to publishing is streamlined. I can focus on content rather than managing infrastructure.</p>
<h2>What I&#39;ll Write About</h2>
<p>I plan to cover three main areas:</p>
<p><strong>Machine Learning</strong>: Model architectures, training methodologies, inference optimization, evaluation frameworks, and what neural networks actually learn. I&#39;m particularly interested in language models and the mechanics of how they process and generate text.</p>
<p><strong>Technology</strong>: Side projects and experiments that catch my attention. The tools I build for fun, weekend hacks that solve interesting problems, and personal projects that make me think differently about what&#39;s possible with code.</p>
<p><strong>Philosophy</strong>: Philosophy of mind, language, science, and logic. Exploring questions about meaning, knowledge representation, understanding, and what it means for machines to &quot;know&quot; something.</p>
<h2>Getting Started</h2>
<p>This is just the beginning. I&#39;m looking forward to sharing more substantial posts about the projects and ideas I&#39;m working on.</p>
<p>Thanks for reading!</p>
]]></content:encoded>
    </item>
  </channel>
</rss>